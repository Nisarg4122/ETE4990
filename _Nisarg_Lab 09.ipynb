{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f26ddd3",
   "metadata": {},
   "source": [
    "\n",
    "# Lab 9: Build a Log Aggregator\n",
    "\n",
    "In this lab, you will create your own log generator, build a command-line utility that scans log files, summarizes their contents, and provides insight into system behavior. Data structures to track log message levels such as `INFO`, `WARNING`, `ERROR`, and `CRITICAL`.\n",
    "\n",
    "This lab reinforces:\n",
    "- File I/O\n",
    "- Pattern recognition (regex)\n",
    "- Dictionaries and counters\n",
    "- Functions and modularity\n",
    "- CLI arguments, logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5ee8a",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Create Log files (20%)\n",
    "Using the the following example log format below create a **python file** that will log errors In a structured tree format \n",
    "\n",
    "You will find examples in the folder called Logs that you can use to build your program.\n",
    "\n",
    "Remember set of logs should have a varied levels of log entries (`INFO`, `WARNING`, `ERROR`, `CRITICAL`) and tailored message types for different service components.\n",
    "You must create 5 structured logs here are some examples:\n",
    "\n",
    "    sqldb\n",
    "    ui\n",
    "    frontend.js\n",
    "    backend.js\n",
    "    frontend.flask\n",
    "    backend.flask\n",
    "\n",
    "You may use chat GPT to create sample outputs NOT THE LOGS. IE:\n",
    "\n",
    "    System failure\n",
    "    Database corruption\n",
    "    Disk failure detected\n",
    "    Database corruption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec9ba30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Step 1: Make sure Logs folder exists\n",
    "os.makedirs(\"Logs\", exist_ok=True)\n",
    "\n",
    "# Step 2: Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n",
    "    filename='Logs/smart_home_logs.log',\n",
    "    filemode='w',\n",
    "    force=True\n",
    ")\n",
    "\n",
    "# Step 3: Define loggers (parent and children)\n",
    "main_logger = logging.getLogger(\"home_system\")\n",
    "security_logger = logging.getLogger(\"home_system.security\")\n",
    "heating_logger = logging.getLogger(\"home_system.heating\")\n",
    "lighting_logger = logging.getLogger(\"home_system.lighting\")\n",
    "network_logger = logging.getLogger(\"home_system.network\")\n",
    "\n",
    "# Step 4: Log events with different levels\n",
    "main_logger.info(\"Smart home system initialized\")\n",
    "security_logger.warning(\"Motion detected at the front door\")\n",
    "heating_logger.error(\"Temperature sensor not responding\")\n",
    "lighting_logger.info(\"Living room lights turned off automatically\")\n",
    "network_logger.critical(\"Wi-Fi connection lost â€” attempting reconnection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5255ab",
   "metadata": {},
   "source": [
    "\n",
    "### Example Log Format\n",
    "\n",
    "You will work with logs that follow this simplified structure:\n",
    "\n",
    "```\n",
    "2025-04-11 23:20:36,913 | my_app | INFO | Request completed\n",
    "2025-04-11 23:20:36,914 | my_app.utils | ERROR | Unhandled exception\n",
    "2025-04-11 23:20:36,914 | my_app.utils.db | CRITICAL | Disk failure detected\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659dfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5f6e84",
   "metadata": {},
   "source": [
    "## Part 2: Logging the Log File (40%)\n",
    "    New File\n",
    "### Part 2a: Read the Log File (see lab 7) (10%)\n",
    "\n",
    "\n",
    "Write a function to read the contents of a log file into a list of lines. Handle file errors gracefully.\n",
    "\n",
    "### Part 2b: Parse Log Lines (see code below if you get stuck) (10%)\n",
    "\n",
    "Use a regular expression to extract:\n",
    "- Timestamp\n",
    "- Log name\n",
    "- Log level\n",
    "- Message\n",
    "\n",
    "### Part 2c: Count Log Levels (20%)\n",
    "\n",
    "Create a function to count how many times each log level appears. Store the results in a dictionary. Then output it as a Json File\n",
    "You may pick your own format but here is an example. \n",
    "```python\n",
    "{\n",
    "    \"INFO\": \n",
    "    {\n",
    "        \"Request completed\": 42, \n",
    "        \"Heartbeat OK\": 7\n",
    "    }\n",
    "\n",
    "    \"WARNING\":\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc631f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your python file here don't for get to upload it with your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a49be070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log summary written to 'log_summary.json'\n",
      "defaultdict(<class 'int'>, {'INFO': 2, 'WARNING': 1, 'ERROR': 1, 'CRITICAL': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "\n",
    "def read_log_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            return file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "    return []\n",
    "\n",
    "def parse_log_line(line):\n",
    "    pattern = r'^(.*?)\\s+\\|\\s+(.*?)\\s+\\|\\s+(.*?)\\s+\\|\\s+(.*)$'\n",
    "    match = re.match(pattern, line.strip())\n",
    "    if match:\n",
    "        return {\n",
    "            \"timestamp\": match.group(1),\n",
    "            \"log_name\": match.group(2),\n",
    "            \"level\": match.group(3),\n",
    "            \"message\": match.group(4)\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def count_log_levels(filepath, output_json=\"log_summary.json\"):\n",
    "    lines = read_log_file(filepath)\n",
    "    level_counts = defaultdict(int)\n",
    "\n",
    "    for line in lines:\n",
    "        parsed = parse_log_line(line)\n",
    "        if parsed:\n",
    "            level = parsed[\"level\"]\n",
    "            level_counts[level] += 1\n",
    "\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(level_counts, f, indent=4)\n",
    "\n",
    "    print(f\"Log summary written to '{output_json}'\")\n",
    "    return level_counts\n",
    "\n",
    "log_file_path = \"Logs/smart_home_logs.log\"\n",
    "summary = count_log_levels(log_file_path)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045c30f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Generate Summary Report (40%)\n",
    "    New File\n",
    "### Step 3a (20%):\n",
    " Develop a function that continuously monitors your JSON file(s) and will print a real-time summary of log activity. It should keep count of the messages grouped by log level (INFO, WARNING, ERROR, CRITICAL) and display only the critical messages. (I.e. If new data comes in the summary will change and a new critical message will be printed)\n",
    " - note: do not reprocess the entire file on each update.  \n",
    "\n",
    "### Step 3a: Use a Matplotlib (Lecture 10) (20%)\n",
    "Develop a function that continuously monitors your JSON file(s) and will graph in real-time a bar or pie plot of each of the errors.  (a graph for each log level). \n",
    "- The graph should show the distribution of log messages by level  (INFO, WARNING, ERROR, CRITICAL)  \n",
    "\n",
    "\n",
    "### Critical notes:\n",
    "- Your code mus use Daemon Threads (Lecture 14)\n",
    "- 3a and 3b do not need to run at the same time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c39868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def monitor_log_summary(json_file_path):\n",
    "    last_seen = {}\n",
    "\n",
    "    def watch_file():\n",
    "        print(\"Starting log monitor ...\")\n",
    "        while True:\n",
    "            if os.path.exists(json_file_path):\n",
    "                with open(json_file_path, \"r\") as f:\n",
    "                    try:\n",
    "                        data = json.load(f)\n",
    "                    except json.JSONDecodeError:\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "\n",
    "                # Compare with last seen and print updates\n",
    "                for level, count in data.items():\n",
    "                    if level == \"CRITICAL\":\n",
    "                        last_count = last_seen.get(level, 0)\n",
    "                        if count > last_count:\n",
    "                            print(f\"New CRITICAL log detected! Count: {count}\")\n",
    "                    last_seen[level] = count\n",
    "\n",
    "            time.sleep(2)  # Polling every 2 seconds\n",
    "\n",
    "    t = threading.Thread(target=watch_file, daemon=True)\n",
    "    t.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31040706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def live_plot_log_summary(json_file_path):\n",
    "    \"\"\"Continuously update a bar chart of log levels from a JSON file.\"\"\"\n",
    "    plt.ion()  # interactive mode on\n",
    "\n",
    "    def plot_loop():\n",
    "        while True:\n",
    "            if os.path.exists(json_file_path):\n",
    "                with open(json_file_path, \"r\") as f:\n",
    "                    try:\n",
    "                        data = json.load(f)\n",
    "                    except json.JSONDecodeError:\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "\n",
    "                levels = list(data.keys())\n",
    "                counts = list(data.values())\n",
    "\n",
    "                plt.clf()\n",
    "                plt.bar(levels, counts)\n",
    "                plt.title(\"Log Level Summary\")\n",
    "                plt.xlabel(\"Log Level\")\n",
    "                plt.ylabel(\"Count\")\n",
    "                plt.pause(2)  # update every 2 seconds\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "    t = threading.Thread(target=plot_loop, daemon=True)\n",
    "    t.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db3bf356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting log monitor (daemon)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# OR: Run live graph (run one at a time as per instructions)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# live_plot_log_summary(json_path)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Keep main thread alive\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Path to your log summary JSON\n",
    "json_path = \"Logs/log_summary.json\"\n",
    "\n",
    "# Run real-time summary monitoring\n",
    "monitor_log_summary(json_path)\n",
    "\n",
    "# OR: Run live graph (run one at a time as per instructions)\n",
    "# live_plot_log_summary(json_path)\n",
    "\n",
    "# Keep main thread alive\n",
    "while True:\n",
    "    time.sleep(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
